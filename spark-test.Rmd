---
title: "Spark"
output: html_document
---

```{r}
library(tidyverse)
#devtools::install_github("rstudio/sparklyr")
library(sparklyr)
```

Which versions are available for download?

```{r}
spark_versions()%>%arrange(spark)%>%tail
```

Install Spark and NYC Flights

```{r}
#spark_install(version = "2.4.4")
#install.packages("nycflights13")
```

Connect with local Spark

```{r}
sc <- spark_connect(master = "local")
```

Loads table into Spark 

```{r}
flights_tbl <- copy_to(sc, nycflights13::flights, "flights")
src_tbls(sc)
```

Use sdf_nrow() to count rows

```{r}
flights_tbl %>% sparklyr::sdf_nrow()
```

```{r}
flights_tbl %>% filter(dep_delay == 2) %>% sdf_nrow()
```

Run group by and filter

```{r}
delay <- flights_tbl %>% 
  group_by(tailnum) %>%
  summarise(count = n(), dist = mean(distance), delay = mean(arr_delay)) %>%
  filter(count > 20, dist < 2000, !is.na(delay)) %>%
  collect
```

Write out as csv

```{r}
csv_dir <- "flights_csv"
spark_write_csv(flights_tbl, csv_dir)
#iris_csv_tbl <- spark_read_csv(sc, "iris_csv", temp_csv)
```

Read it

```{r}
spark_read_csv(sc, "flights_csv", csv_dir)
```

Or as parquet

```{r}
parquet_dir <- "flights_parquet"
spark_write_parquet(flights_tbl, parquet_dir)
#iris_csv_tbl <- spark_read_csv(sc, "iris_csv", temp_csv)
```

Simulate a large file (13 regions)

```{r}
samples <- 10e7
df_nf13 <- tibble(buying_region=sample.int(13,samples,replace=T),
                  selling_region=sample.int(13,samples,replace=T),
                  value=10^runif(samples,3,7))
```

```{r}
nf13_tbl <- copy_to(sc, df_nf13, "nf13")
```


```{r}
spark_write_csv(nf13_tbl, "nf13_csv")
```


Plot Delays

```{r}
delay %>%
  ggplot(aes(dist, delay)) +
  geom_point(aes(size = count), alpha = 1/2) +
  geom_smooth() +
  scale_size_area(max_size = 2)
```

```{r}
sparklyr::spark_disconnect(sc)
```

